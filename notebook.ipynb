{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for class imbalance in OpenML Collection 99...\n",
      "\n",
      "Datasets with class imbalance greater than the threshold:\n",
      "\n",
      "Dataset ID: 40983\n",
      "Dataset Name: wilt\n",
      "Max Class Proportion: 94.61%\n",
      "Class Distribution:\n",
      "  1: 94.61%\n",
      "  2: 5.39%\n",
      "\n",
      "Dataset ID: 38\n",
      "Dataset Name: sick\n",
      "Max Class Proportion: 93.88%\n",
      "Class Distribution:\n",
      "  negative: 93.88%\n",
      "  sick: 6.12%\n",
      "\n",
      "Dataset ID: 1487\n",
      "Dataset Name: ozone-level-8hr\n",
      "Max Class Proportion: 93.69%\n",
      "Class Distribution:\n",
      "  1: 93.69%\n",
      "  2: 6.31%\n",
      "\n",
      "Dataset ID: 1068\n",
      "Dataset Name: pc1\n",
      "Max Class Proportion: 93.06%\n",
      "Class Distribution:\n",
      "  False: 93.06%\n",
      "  True: 6.94%\n",
      "\n",
      "Dataset ID: 40994\n",
      "Dataset Name: climate-model-simulation-crashes\n",
      "Max Class Proportion: 91.48%\n",
      "Class Distribution:\n",
      "  0: 8.52%\n",
      "  1: 91.48%\n",
      "\n",
      "Dataset ID: 1050\n",
      "Dataset Name: pc3\n",
      "Max Class Proportion: 89.76%\n",
      "Class Distribution:\n",
      "  False: 89.76%\n",
      "  True: 10.24%\n",
      "\n",
      "Dataset ID: 1461\n",
      "Dataset Name: bank-marketing\n",
      "Max Class Proportion: 88.30%\n",
      "Class Distribution:\n",
      "  1: 88.30%\n",
      "  2: 11.70%\n",
      "\n",
      "Dataset ID: 1049\n",
      "Dataset Name: pc4\n",
      "Max Class Proportion: 87.79%\n",
      "Class Distribution:\n",
      "  False: 87.79%\n",
      "  True: 12.21%\n",
      "\n",
      "Dataset ID: 40978\n",
      "Dataset Name: Internet-Advertisements\n",
      "Max Class Proportion: 86.00%\n",
      "Class Distribution:\n",
      "  ad: 14.00%\n",
      "  noad: 86.00%\n",
      "\n",
      "Dataset ID: 40701\n",
      "Dataset Name: churn\n",
      "Max Class Proportion: 85.86%\n",
      "Class Distribution:\n",
      "  0: 85.86%\n",
      "  1: 14.14%\n",
      "\n",
      "Dataset ID: 1067\n",
      "Dataset Name: kc1\n",
      "Max Class Proportion: 84.54%\n",
      "Class Distribution:\n",
      "  False: 84.54%\n",
      "  True: 15.46%\n",
      "\n",
      "Dataset ID: 1053\n",
      "Dataset Name: jm1\n",
      "Max Class Proportion: 80.65%\n",
      "Class Distribution:\n",
      "  False: 80.65%\n",
      "  True: 19.35%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import openml\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def check_class_imbalance(threshold=0.8):\n",
    "    # Get datasets from OpenML Collection with ID 99\n",
    "    datasets = openml.study.get_suite(99)  # Suite ID 99\n",
    "\n",
    "    # List to store datasets with imbalance\n",
    "    imbalanced_datasets = []\n",
    "\n",
    "    print(\"Checking for class imbalance in OpenML Collection 99...\\n\")\n",
    "\n",
    "    for dataset_id in datasets.data:\n",
    "        try:\n",
    "            # Download the dataset\n",
    "            dataset = openml.datasets.get_dataset(dataset_id)\n",
    "            X, y, _, attributes = dataset.get_data(\n",
    "                target=dataset.default_target_attribute\n",
    "            )\n",
    "\n",
    "            # Ensure the target variable is categorical\n",
    "            if not isinstance(y, pd.Categorical):\n",
    "                y = pd.Categorical(y)\n",
    "\n",
    "            # Calculate class distribution\n",
    "            class_counts = y.value_counts()  # Absolute counts\n",
    "            total = class_counts.sum()  # Total number of instances\n",
    "            class_proportions = class_counts / total  # Normalized proportions\n",
    "            max_class_proportion = class_proportions.max()\n",
    "\n",
    "            # Check for imbalance\n",
    "            if max_class_proportion >= threshold:\n",
    "                imbalanced_datasets.append(\n",
    "                    {\n",
    "                        \"Dataset ID\": dataset_id,\n",
    "                        \"Dataset Name\": dataset.name,\n",
    "                        \"Max Class Proportion\": max_class_proportion,\n",
    "                        \"Class Distribution\": class_proportions.to_dict(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing dataset {dataset_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Sort the imbalanced datasets by max class proportion in descending order\n",
    "    imbalanced_datasets = sorted(\n",
    "        imbalanced_datasets, key=lambda x: x[\"Max Class Proportion\"], reverse=True\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    if imbalanced_datasets:\n",
    "        print(\"Datasets with class imbalance greater than the threshold:\\n\")\n",
    "        for dataset in imbalanced_datasets:\n",
    "            print(f\"Dataset ID: {dataset['Dataset ID']}\")\n",
    "            print(f\"Dataset Name: {dataset['Dataset Name']}\")\n",
    "            print(f\"Max Class Proportion: {dataset['Max Class Proportion']:.2%}\")\n",
    "            print(\"Class Distribution:\")\n",
    "            for cls, proportion in dataset[\"Class Distribution\"].items():\n",
    "                print(f\"  {cls}: {proportion:.2%}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No datasets found with class imbalance greater than the threshold.\")\n",
    "\n",
    "    return imbalanced_datasets\n",
    "\n",
    "\n",
    "# Run the script\n",
    "imbalanced_datasets = check_class_imbalance(threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "def MySmote(X, y, minority_class, k_neighbors=5, n_samples=100, max_distance=None):\n",
    "    \"\"\"\n",
    "    Implementa uma versão personalizada do SMOTE.\n",
    "\n",
    "    Parâmetros:\n",
    "    - X (array-like): Dados de entrada (features).\n",
    "    - y (array-like): Rótulos correspondentes.\n",
    "    - minority_class: Classe minoritária (exemplo: 1 ou \"positive\").\n",
    "    - k_neighbors (int): Número de vizinhos a considerar para gerar novos exemplos.\n",
    "    - n_samples (int): Número de exemplos sintéticos a serem gerados.\n",
    "    - max_distance (float): Distância máxima permitida para geração de exemplos sintéticos.\n",
    "\n",
    "    Retorna:\n",
    "    - X_new (numpy.ndarray): Conjunto de dados com exemplos sintéticos adicionados.\n",
    "    - y_new (numpy.ndarray): Rótulos correspondentes ao novo conjunto de dados.\n",
    "    \"\"\"\n",
    "    # Separar classe minoritária\n",
    "    X_minority = X[y == minority_class]\n",
    "    if len(X_minority) == 0:\n",
    "        raise ValueError(\"Não há dados da classe minoritária.\")\n",
    "\n",
    "    # Identificar k-vizinhos mais próximos na classe minoritária\n",
    "    nn = NearestNeighbors(n_neighbors=k_neighbors + 1).fit(X_minority)\n",
    "    distances, indices = nn.kneighbors(X_minority)\n",
    "\n",
    "    # Criar novos exemplos sintéticos\n",
    "    synthetic_samples = []\n",
    "    for _ in range(n_samples):\n",
    "        # Selecionar um ponto minoritário aleatoriamente\n",
    "        idx = np.random.randint(0, X_minority.shape[0])\n",
    "        x_original = X_minority[idx]\n",
    "\n",
    "        # Selecionar um vizinho próximo aleatório\n",
    "        neighbor_idx = np.random.choice(\n",
    "            indices[idx][1:]\n",
    "        )  # Ignorar o próprio ponto (primeiro vizinho)\n",
    "        x_neighbor = X_minority[neighbor_idx]\n",
    "\n",
    "        # Gerar exemplo sintético no segmento entre x_original e x_neighbor\n",
    "        diff = x_neighbor - x_original\n",
    "        gap = np.random.rand()  # Número aleatório entre 0 e 1\n",
    "        synthetic_point = x_original + gap * diff\n",
    "\n",
    "        # Restringir exemplos com base na distância máxima (se fornecido)\n",
    "        if max_distance is not None:\n",
    "            dist = np.linalg.norm(synthetic_point - x_original)\n",
    "            if dist > max_distance:\n",
    "                continue  # Pula esta geração\n",
    "\n",
    "        synthetic_samples.append(synthetic_point)\n",
    "\n",
    "    # Adicionar exemplos sintéticos aos dados originais\n",
    "    X_new = np.vstack([X, synthetic_samples])\n",
    "    y_new = np.hstack([y, [minority_class] * len(synthetic_samples)])\n",
    "\n",
    "    return X_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(\n",
    "        self, feature=None, threshold=None, left=None, right=None, *, value=None\n",
    "    ):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def gini(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities**2)\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        best_gini = 1.0\n",
    "        best_feature, best_threshold = None, None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            X_feature = X[:, feature]\n",
    "            sorted_indices = np.argsort(X_feature)\n",
    "            X_sorted = X_feature[sorted_indices]\n",
    "            y_sorted = y[sorted_indices]\n",
    "\n",
    "            # Identify potential split points\n",
    "            for i in range(1, len(y_sorted)):\n",
    "                if y_sorted[i] != y_sorted[i - 1]:\n",
    "                    threshold = (X_sorted[i] + X_sorted[i - 1]) / 2\n",
    "                    left_mask = X_sorted < threshold\n",
    "                    y_left = y_sorted[left_mask]\n",
    "                    y_right = y_sorted[~left_mask]\n",
    "\n",
    "                    if len(y_left) == 0 or len(y_right) == 0:\n",
    "                        continue\n",
    "\n",
    "                    gini_left = self.gini(y_left)\n",
    "                    gini_right = self.gini(y_right)\n",
    "                    gini = (len(y_left) * gini_left + len(y_right) * gini_right) / len(\n",
    "                        y_sorted\n",
    "                    )\n",
    "\n",
    "                    if gini < best_gini:\n",
    "                        best_gini = gini\n",
    "                        best_feature = feature\n",
    "                        best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        num_samples, _ = X.shape\n",
    "        if (\n",
    "            self.max_depth is not None and depth >= self.max_depth\n",
    "        ) or num_samples < self.min_samples_split:\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return TreeNode(value=leaf_value)\n",
    "\n",
    "        feature, threshold = self.best_split(X, y)\n",
    "        if feature is None:\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return TreeNode(value=leaf_value)\n",
    "\n",
    "        # Split the data\n",
    "        left_indices = X[:, feature] < threshold\n",
    "        X_left, y_left = X[left_indices], y[left_indices]\n",
    "        X_right, y_right = X[~left_indices], y[~left_indices]\n",
    "\n",
    "        if len(y_left) == 0 or len(y_right) == 0:\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return TreeNode(value=leaf_value)\n",
    "\n",
    "        # Recursively build the left and right subtrees\n",
    "        left = self.build_tree(X_left, y_left, depth + 1)\n",
    "        right = self.build_tree(X_right, y_right, depth + 1)\n",
    "        return TreeNode(feature, threshold, left, right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.build_tree(X, y)\n",
    "\n",
    "    def predict_sample(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature] < node.threshold:\n",
    "            return self.predict_sample(x, node.left)\n",
    "        else:\n",
    "            return self.predict_sample(x, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_sample(x, self.root) for x in X])\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        max_features=\"sqrt\",\n",
    "        smote=False,\n",
    "        random_state=None,\n",
    "    ):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.smote = smote\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "\n",
    "    def bootstrap_sample(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        X_sample, y_sample = X[indices], y[indices]\n",
    "\n",
    "        # Ensure sampled data is still numeric\n",
    "        if X_sample.dtype == \"O\" or y_sample.dtype == \"O\":\n",
    "            X_sample, y_sample = convert_to_numeric(X_sample, y_sample)\n",
    "\n",
    "        return X_sample, y_sample\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert to NumPy arrays if they are pandas structures\n",
    "        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "\n",
    "        # Convert to numeric if necessary\n",
    "        if X.dtype == \"O\" or y.dtype == \"O\":\n",
    "            X, y = convert_to_numeric(X, y)\n",
    "\n",
    "        # Apply SMOTE if enabled\n",
    "        if self.smote:\n",
    "            sm = SMOTE(sampling_strategy=1, random_state=self.random_state)\n",
    "\n",
    "            # sm = RandomUnderSampler(random_state=self.random_state)\n",
    "            # sm = SMOTEENN(random_state=self.random_state)\n",
    "            # sm = SMOTETomek(random_state=self.random_state)\n",
    "            # sm = ADASYN(random_state=self.random_state)\n",
    "            X, y = sm.fit_resample(X, y)\n",
    "\n",
    "        self.trees = []\n",
    "        n_features_total = X.shape[1]\n",
    "\n",
    "        # Determine the number of features to consider at each split\n",
    "        if self.max_features == \"sqrt\":\n",
    "            max_features = int(np.sqrt(n_features_total))\n",
    "        elif self.max_features == \"log2\":\n",
    "            max_features = int(np.log2(n_features_total))\n",
    "        elif isinstance(self.max_features, int):\n",
    "            max_features = self.max_features\n",
    "        elif self.max_features.lower() == \"auto\":\n",
    "            max_features = n_features_total\n",
    "        else:\n",
    "            raise ValueError(\"RandomForest.fit: Invalid value for max_features\")\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTree(\n",
    "                max_depth=self.max_depth, min_samples_split=self.min_samples_split\n",
    "            )\n",
    "            if self.smote:\n",
    "                X_sample, y_sample = X, y\n",
    "            else:\n",
    "                X_sample, y_sample = self.bootstrap_sample(X, y)\n",
    "\n",
    "            # Select random subset of features\n",
    "            features = np.random.choice(n_features_total, max_features, replace=False)\n",
    "\n",
    "            # Fit the tree on the sampled data with selected features\n",
    "            tree.fit(X_sample[:, features], y_sample)\n",
    "\n",
    "            tree.features = features\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Convert to NumPy array if it's a pandas structure\n",
    "        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "            X = X.values\n",
    "\n",
    "        tree_preds = []\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            try:\n",
    "                preds = tree.predict(X[:, tree.features])\n",
    "                tree_preds.append(preds)\n",
    "            except Exception as e:\n",
    "                print(f\"RandomForest.predict: Error predicting with tree {i+1}: {e}\")\n",
    "                raise e\n",
    "\n",
    "        tree_preds = np.array(tree_preds).T  # Shape: (n_samples, n_estimators)\n",
    "        y_pred = [Counter(row).most_common(1)[0][0] for row in tree_preds]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "\n",
    "# Helper function\n",
    "def convert_to_numeric(X, y):\n",
    "    # Convert X to float\n",
    "    if X.dtype == \"O\":\n",
    "        X = X.astype(float)\n",
    "\n",
    "    # Convert y to integer\n",
    "    if y.dtype == \"O\":\n",
    "        y = y.astype(int)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ID: 40994\n",
      "\n",
      "No non-numeric features to encode.\n",
      "\n",
      "Encoded Target dtype: int8\n",
      "Target Categories and Codes:\n",
      " {'0': np.int8(0), '1': np.int8(1)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SMOTE Accuracy: 0.86\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.89      0.52         9\n",
      "           1       0.99      0.86      0.92        99\n",
      "\n",
      "    accuracy                           0.86       108\n",
      "   macro avg       0.68      0.87      0.72       108\n",
      "weighted avg       0.94      0.86      0.89       108\n",
      "\n",
      "\n",
      "Without SMOTE Accuracy: 0.93\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.11      0.20         9\n",
      "           1       0.93      1.00      0.96        99\n",
      "\n",
      "    accuracy                           0.93       108\n",
      "   macro avg       0.96      0.56      0.58       108\n",
      "weighted avg       0.93      0.93      0.90       108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import openml  # Ensure openml is imported\n",
    "\n",
    "# Define your custom RandomForest class here or ensure it's imported\n",
    "# from your_random_forest_module import RandomForest\n",
    "\n",
    "# Load the datasets from OpenML Collection 99\n",
    "dataset_id = imbalanced_datasets[4][\"Dataset ID\"]\n",
    "print(\"Dataset ID:\", dataset_id)\n",
    "data = openml.datasets.get_dataset(dataset_id)\n",
    "\n",
    "X, y, _, _ = data.get_data(\n",
    "    dataset_format=\"dataframe\", target=data.default_target_attribute\n",
    ")\n",
    "\n",
    "# Combine X and y for consistent row removal\n",
    "combined_data = pd.concat([X, y], axis=1)\n",
    "\n",
    "combined_data_clean = combined_data.dropna()\n",
    "\n",
    "# Separate X_clean and y_clean\n",
    "X_clean = combined_data_clean.drop(\n",
    "    columns=[y.name]\n",
    ")  # Replace y.name with the actual target column name if different\n",
    "y_clean = combined_data_clean[y.name]\n",
    "\n",
    "# Update X and y to cleaned versions\n",
    "X, y = X_clean, y_clean\n",
    "\n",
    "# Identify non-numeric columns in X\n",
    "non_numeric_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "# Encode categorical features by converting them to categorical codes\n",
    "if len(non_numeric_cols) > 0:\n",
    "    for col in non_numeric_cols:\n",
    "        X[col] = X[col].astype(\"category\").cat.codes\n",
    "        print(f\"\\nEncoded '{col}' with codes:\\n\", X[col].unique())\n",
    "    print(\"\\nAfter Encoding Feature dtypes:\\n\", X.dtypes)\n",
    "else:\n",
    "    print(\"\\nNo non-numeric features to encode.\")\n",
    "\n",
    "# Encode target variable using Pandas Categorical Codes if it's non-numeric\n",
    "if y.dtype == \"object\" or y.dtype.name == \"category\":\n",
    "    y = y.astype(\"category\")\n",
    "    y_codes = y.cat.codes\n",
    "    print(\"\\nEncoded Target dtype:\", y_codes.dtype)\n",
    "    print(\n",
    "        \"Target Categories and Codes:\\n\", dict(zip(y.cat.categories, y_codes.unique()))\n",
    "    )\n",
    "    y = y_codes\n",
    "\n",
    "max_accuracy = 0\n",
    "max_random_state = 0\n",
    "\n",
    "random_state = 99\n",
    "\n",
    "# Split the data with stratification to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=random_state, stratify=y\n",
    ")\n",
    "\n",
    "# Initialize RandomForest with SMOTE\n",
    "rf = RandomForest(\n",
    "    n_estimators=15,\n",
    "    max_depth=5,\n",
    "    smote=True,\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.sum(predictions == y_test) / len(y_test)\n",
    "print(f\"\\nSMOTE Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "rf = RandomForest(\n",
    "    n_estimators=15,\n",
    "    max_depth=5,\n",
    "    smote=False,\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.sum(predictions == y_test) / len(y_test)\n",
    "print(f\"\\nWithout SMOTE Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
