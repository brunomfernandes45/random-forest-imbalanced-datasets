{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IACEC Project: Random Forest on Small Imbalanced Datasets**\n",
    "\n",
    "## **Group 07: G07**\n",
    "\n",
    "### **Authors:**\n",
    "- **Bruno Fernandes**: [up202108871](up202108871@up.pt)\n",
    "- **Hugo Abelheira**: [up202409899](up202409899@up.pt)\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "### **Context**\n",
    "\n",
    "This project was developed in for the course \"Introduction to Algorithms and Data Structures\" (IACEC) of the Master's in Data Science and Engineering (MDSE) at the Faculty of Engineering of the University of Porto (FEUP).\n",
    "\n",
    "In this project, we aim to explore the Random Forest algorithm in the context of small imbalanced datasets. We will analyze the performance of the algorithm in this context and propose some strategies to improve it.\n",
    "\n",
    "### **Issue At Hand**\n",
    "\n",
    "We chose this topic because we believe that the Random Forest algorithm is a powerful tool for binary classification tasks, but it can be sensitive to imbalanced datasets. We want to understand how the algorithm behaves in this context and how we can improve its performance.\n",
    "\n",
    "### **Dataset Characteristics**\n",
    "\n",
    "The implementation is tested on datasets from the OpenML-CC18 collection with:\n",
    "- Binary classification tasks\n",
    "- Significant class imbalance (minority class < 20%)\n",
    "\n",
    "### **Proposed Adjustments**\n",
    "\n",
    "We propose, implement, and test SMOTE oversampling techniques instead of the bootstrap sampling used in the default Random Forest algorithm. This technique aims to balance the classes in the training set by generating synthetic samples of the minority class. By applying SMOTE during the training phase, we address the fundamental issue of class imbalance at the data level, allowing each tree in the Random Forest to learn from a more balanced dataset.\n",
    "\n",
    "In our implementation, we focus on maximizing recall for the positive (minority) class, as in many real-world applications with imbalanced datasets, it is more important to identify positive cases correctly (minimize false negatives) even at the cost of increased false positives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openml\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay, auc,\n",
    "                             classification_report, confusion_matrix,\n",
    "                             roc_curve)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Getting the Imbalanced Datasets from OpenML-CC18**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The check_class_imbalance function is used to identify and list the most imbalanced datasets in the OpenML 99 collection, based on a specified threshold for class imbalance. This step allows us to select the datasets most suitable for the purpose of our problem, which involves addressing and correcting class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_imbalance(threshold=0.8):\n",
    "    print(\"Loading OpenML Collection 99 datasets...\")\n",
    "    \n",
    "    # Get datasets from OpenML Collection with ID 99\n",
    "    datasets = openml.study.get_suite(99)  # Suite ID 99\n",
    "\n",
    "    # List to store datasets with imbalance\n",
    "    imbalanced_datasets = []\n",
    "\n",
    "    print(\"Checking for class imbalance in OpenML Collection 99...\\n\")\n",
    "\n",
    "    for dataset_id in datasets.data:\n",
    "        try:\n",
    "            # Download the dataset\n",
    "            dataset = openml.datasets.get_dataset(dataset_id)\n",
    "            X, y, _, attributes = dataset.get_data(\n",
    "                target=dataset.default_target_attribute\n",
    "            )\n",
    "\n",
    "            # Ensure the target variable is categorical\n",
    "            if not isinstance(y, pd.Categorical):\n",
    "                y = pd.Categorical(y)\n",
    "\n",
    "            # Calculate class distribution\n",
    "            class_counts = y.value_counts()  # Absolute counts\n",
    "            total = class_counts.sum()  # Total number of instances\n",
    "            class_proportions = class_counts / total  # Normalized proportions\n",
    "            max_class_proportion = class_proportions.max()\n",
    "\n",
    "            # Check for imbalance\n",
    "            if max_class_proportion >= threshold:\n",
    "                imbalanced_datasets.append(\n",
    "                    {\n",
    "                        \"Dataset ID\": dataset_id,\n",
    "                        \"Dataset Name\": dataset.name,\n",
    "                        \"Max Class Proportion\": max_class_proportion,\n",
    "                        \"Class Distribution\": class_proportions.to_dict(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing dataset {dataset_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Sort the imbalanced datasets by max class proportion in descending order\n",
    "    imbalanced_datasets = sorted(\n",
    "        imbalanced_datasets, key=lambda x: x[\"Max Class Proportion\"], reverse=True\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    if imbalanced_datasets:\n",
    "        print(\"Datasets with class imbalance greater than the threshold:\\n\")\n",
    "        for dataset in imbalanced_datasets:\n",
    "            print(f\"Dataset ID: {dataset['Dataset ID']}\")\n",
    "            print(f\"Dataset Name: {dataset['Dataset Name']}\")\n",
    "            print(f\"Max Class Proportion: {dataset['Max Class Proportion']:.2%}\")\n",
    "            print(\"Class Distribution:\")\n",
    "            for cls, proportion in dataset[\"Class Distribution\"].items():\n",
    "                print(f\"  {cls}: {proportion:.2%}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No datasets found with class imbalance greater than the threshold.\")\n",
    "\n",
    "    return imbalanced_datasets\n",
    "\n",
    "\n",
    "# Run the script\n",
    "imbalanced_datasets = check_class_imbalance(threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Choosing the Datasets for the Benchmark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalanced_datasets = [imbalanced_datasets[0], imbalanced_datasets[2], imbalanced_datasets[4], imbalanced_datasets[5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forest Implementation**\n",
    "\n",
    "Our implementation includes a parameter `smote` that allows choosing between the original bootstrap sampling and SMOTE oversampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(\n",
    "        self, feature=None, threshold=None, left=None, right=None, *, value=None\n",
    "    ):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def gini(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities**2)\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        best_gini = 1.0\n",
    "        best_feature, best_threshold = None, None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            X_feature = X[:, feature]\n",
    "            sorted_indices = np.argsort(X_feature)\n",
    "            X_sorted = X_feature[sorted_indices]\n",
    "            y_sorted = y[sorted_indices]\n",
    "\n",
    "            # Identify potential split points\n",
    "            for i in range(1, len(y_sorted)):\n",
    "                if y_sorted[i] != y_sorted[i - 1]:\n",
    "                    threshold = (X_sorted[i] + X_sorted[i - 1]) / 2\n",
    "                    left_mask = X_sorted < threshold\n",
    "                    y_left = y_sorted[left_mask]\n",
    "                    y_right = y_sorted[~left_mask]\n",
    "\n",
    "                    if len(y_left) == 0 or len(y_right) == 0:\n",
    "                        continue\n",
    "\n",
    "                    gini_left = self.gini(y_left)\n",
    "                    gini_right = self.gini(y_right)\n",
    "                    gini = (len(y_left) * gini_left + len(y_right) * gini_right) / len(\n",
    "                        y_sorted\n",
    "                    )\n",
    "\n",
    "                    if gini < best_gini:\n",
    "                        best_gini = gini\n",
    "                        best_feature = feature\n",
    "                        best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        num_samples, _ = X.shape\n",
    "        if (\n",
    "            self.max_depth is not None and depth >= self.max_depth\n",
    "        ) or num_samples < self.min_samples_split:\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return TreeNode(value=leaf_value)\n",
    "\n",
    "        feature, threshold = self.best_split(X, y)\n",
    "        if feature is None:\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return TreeNode(value=leaf_value)\n",
    "\n",
    "        # Split the data\n",
    "        left_indices = X[:, feature] < threshold\n",
    "        X_left, y_left = X[left_indices], y[left_indices]\n",
    "        X_right, y_right = X[~left_indices], y[~left_indices]\n",
    "\n",
    "        if len(y_left) == 0 or len(y_right) == 0:\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return TreeNode(value=leaf_value)\n",
    "\n",
    "        # Recursively build the left and right subtrees\n",
    "        left = self.build_tree(X_left, y_left, depth + 1)\n",
    "        right = self.build_tree(X_right, y_right, depth + 1)\n",
    "        return TreeNode(feature, threshold, left, right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.build_tree(X, y)\n",
    "\n",
    "    def predict_sample(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature] < node.threshold:\n",
    "            return self.predict_sample(x, node.left)\n",
    "        else:\n",
    "            return self.predict_sample(x, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_sample(x, self.root) for x in X])\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        max_features=\"sqrt\",\n",
    "        smote=False,\n",
    "        random_state=None,\n",
    "    ):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.smote = smote\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "\n",
    "    def bootstrap_sample(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        X_sample, y_sample = X[indices], y[indices]\n",
    "\n",
    "        return X_sample, y_sample\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert to NumPy arrays if they are pandas structures\n",
    "        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "\n",
    "        # Apply SMOTE if enabled\n",
    "        if self.smote:\n",
    "            sm = SMOTE(sampling_strategy=1, random_state=self.random_state)\n",
    "            X, y = sm.fit_resample(X, y)\n",
    "\n",
    "        self.trees = []\n",
    "        n_features_total = X.shape[1]\n",
    "\n",
    "        # Determine the number of features to consider at each split\n",
    "        if self.max_features == \"sqrt\":\n",
    "            max_features = int(np.sqrt(n_features_total))\n",
    "        elif self.max_features == \"log2\":\n",
    "            max_features = int(np.log2(n_features_total))\n",
    "        elif isinstance(self.max_features, int):\n",
    "            max_features = self.max_features\n",
    "        elif self.max_features.lower() == \"auto\":\n",
    "            max_features = n_features_total\n",
    "        else:\n",
    "            raise ValueError(\"RandomForest.fit: Invalid value for max_features\")\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTree(\n",
    "                max_depth=self.max_depth, min_samples_split=self.min_samples_split\n",
    "            )\n",
    "            if self.smote:\n",
    "                X_sample, y_sample = X, y\n",
    "            else:\n",
    "                X_sample, y_sample = self.bootstrap_sample(X, y)\n",
    "\n",
    "            # Select random subset of features\n",
    "            features = np.random.choice(n_features_total, max_features, replace=False)\n",
    "            \n",
    "            # Fit the tree on the sampled data with selected features\n",
    "            tree.fit(X_sample[:, features], y_sample)\n",
    "            \n",
    "            tree.features = features\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Convert to NumPy array if it's a pandas structure\n",
    "        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "            X = X.values\n",
    "\n",
    "        tree_preds = []\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            try:\n",
    "                preds = tree.predict(X[:, tree.features])\n",
    "                tree_preds.append(preds)\n",
    "            except Exception as e:\n",
    "                print(f\"RandomForest.predict: Error predicting with tree {i+1}: {e}\")\n",
    "                raise e\n",
    "\n",
    "        tree_preds = np.array(tree_preds).T  # Shape: (n_samples, n_estimators)\n",
    "        y_pred = [Counter(row).most_common(1)[0][0] for row in tree_preds]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "\n",
    "# Helper function\n",
    "def convert_to_numeric(X, y):\n",
    "    # Convert X to float\n",
    "    if X.dtype == \"O\":\n",
    "        X = X.astype(float)\n",
    "\n",
    "    # Convert y to integer\n",
    "    if y.dtype == \"O\":\n",
    "        y = y.astype(int)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_reports_side_by_side(\n",
    "    y_true,\n",
    "    y_pred_1,\n",
    "    y_pred_2,\n",
    "    titles=[\"Classification Report (Bootstrap)\", \"Classification Report (SMOTE)\"],\n",
    "    cmap=\"coolwarm\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Display two classification reports side by side with a shared colorbar.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (array-like): True labels for both datasets.\n",
    "        y_pred_1 (array-like): Predicted labels for the first dataset.\n",
    "        y_pred_2 (array-like): Predicted labels for the second dataset.\n",
    "        titles (list): Titles for each subplot. Default is [\"Classification Report (Bootstrap)\", \"Classification Report (SMOTE)\"].\n",
    "        cmap (str): Colormap for the heatmaps. Default is \"coolwarm\".\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the classification reports side by side.\n",
    "    \"\"\"\n",
    "    # Set up the figure with two subplots side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6), gridspec_kw={\"wspace\": 0.4})\n",
    "\n",
    "    # Generate the first classification report and plot it\n",
    "    report_dict_1 = classification_report(y_true, y_pred_1, output_dict=True)\n",
    "    report_df_1 = (\n",
    "        pd.DataFrame(report_dict_1)\n",
    "        .transpose()\n",
    "        .drop([\"macro avg\"], errors=\"ignore\")\n",
    "        .round(2)\n",
    "    )\n",
    "    heatmap_1 = sns.heatmap(\n",
    "        report_df_1.iloc[:-1, :-1],\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=cmap,\n",
    "        ax=axes[0],\n",
    "        cbar=False,  # Disable individual colorbars\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"gray\",\n",
    "    )\n",
    "    axes[0].set_title(titles[0], fontsize=14, pad=10)\n",
    "    axes[0].set_xlabel(\"Metrics\", fontsize=12)\n",
    "    axes[0].set_ylabel(\"Classes\", fontsize=12)\n",
    "    axes[0].tick_params(axis=\"x\", labelrotation=45)\n",
    "\n",
    "    # Generate the second classification report and plot it\n",
    "    report_dict_2 = classification_report(y_true, y_pred_2, output_dict=True)\n",
    "    report_df_2 = (\n",
    "        pd.DataFrame(report_dict_2)\n",
    "        .transpose()\n",
    "        .drop([\"macro avg\"], errors=\"ignore\")\n",
    "        .round(2)\n",
    "    )\n",
    "    heatmap_2 = sns.heatmap(\n",
    "        report_df_2.iloc[:-1, :-1],\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=cmap,\n",
    "        ax=axes[1],\n",
    "        cbar=False,  # Disable individual colorbars\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"gray\",\n",
    "    )\n",
    "    axes[1].set_title(titles[1], fontsize=14, pad=10)\n",
    "    axes[1].set_xlabel(\"Metrics\", fontsize=12)\n",
    "    axes[1].tick_params(axis=\"x\", labelrotation=45)\n",
    "\n",
    "    # Add a shared colorbar\n",
    "    # Create a new axis for the colorbar (on the right)\n",
    "    cbar_ax = fig.add_axes([0.92, 0.3, 0.02, 0.4])  # [left, bottom, width, height]\n",
    "    norm = plt.Normalize(\n",
    "        vmin=min(\n",
    "            report_df_1.iloc[:-1, :-1].min().min(),\n",
    "            report_df_2.iloc[:-1, :-1].min().min(),\n",
    "        ),\n",
    "        vmax=max(\n",
    "            report_df_1.iloc[:-1, :-1].max().max(),\n",
    "            report_df_2.iloc[:-1, :-1].max().max(),\n",
    "        ),\n",
    "    )\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])  # Required for ScalarMappable\n",
    "    fig.colorbar(sm, cax=cbar_ax)\n",
    "\n",
    "    # Tight layout and display\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])  # Adjust layout to fit colorbar\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset, random_state=99):\n",
    "    \"\"\"\n",
    "    Function to evaluate Random Forest model using Bootstrap and SMOTE\n",
    "    with comparison plots for Confusion Matrix and ROC-AUC.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (pd.DataFrame): Input dataset with features and target as the last column.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the results in plots and prints execution times.\n",
    "    \"\"\"\n",
    "    # Separate features and target variable into X and y, respectively\n",
    "    X = dataset.drop(columns=[dataset.columns.values[-1]])\n",
    "    y = dataset[dataset.columns.values[-1]]\n",
    "\n",
    "    # Split data into train and test sets (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    #####################\n",
    "    # --- Bootstrap Approach ---\n",
    "    #####################\n",
    "    start_time_bootstrap = time.time()\n",
    "\n",
    "    rf_bootstrap = RandomForest(n_estimators=15, max_depth=5, smote=False, random_state=random_state)\n",
    "    rf_bootstrap.fit(X_train, y_train)\n",
    "    predictions_bootstrap = rf_bootstrap.predict(X_test)\n",
    "\n",
    "    elapsed_bootstrap = time.time() - start_time_bootstrap\n",
    "\n",
    "    # Metrics for Bootstrap\n",
    "    cm_bootstrap = confusion_matrix(y_test, predictions_bootstrap)\n",
    "    y_test_bin_bootstrap = label_binarize(y_test, classes=np.unique(y))\n",
    "    predictions_bin_bootstrap = label_binarize(predictions_bootstrap, classes=np.unique(y))\n",
    "\n",
    "    # Class distribution for Bootstrap\n",
    "    y_train_counts = y_train.value_counts()\n",
    "\n",
    "    #####################\n",
    "    # --- SMOTE Approach ---\n",
    "    #####################\n",
    "    start_time_smote = time.time()\n",
    "\n",
    "    sm = SMOTE(sampling_strategy=1, random_state=random_state)\n",
    "    _, y_resampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    rf_smote = RandomForest(n_estimators=15, max_depth=5, smote=True, random_state=random_state)\n",
    "    rf_smote.fit(X_train, y_train)\n",
    "    predictions_smote = rf_smote.predict(X_test)\n",
    "\n",
    "    elapsed_smote = time.time() - start_time_smote\n",
    "\n",
    "    # Metrics for SMOTE\n",
    "    cm_smote = confusion_matrix(y_test, predictions_smote)\n",
    "    y_test_bin_smote = label_binarize(y_test, classes=np.unique(y))\n",
    "    predictions_bin_smote = label_binarize(predictions_smote, classes=np.unique(y))\n",
    "\n",
    "    # --- Pie Charts for Class Distribution ---\n",
    "    y_resampled_counts = pd.Series(y_resampled).value_counts()\n",
    "\n",
    "    # Adjusting figure size for better visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # Reduced figsize for smaller pie charts\n",
    "\n",
    "    # Pie chart for class distribution before SMOTE\n",
    "    axes[0].pie(\n",
    "        y_train_counts,\n",
    "        labels=y_train_counts.index,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        colors=sns.color_palette(\"muted\"),\n",
    "    )\n",
    "    axes[0].set_title(\"Class Distribution before SMOTE\", fontsize=12)\n",
    "\n",
    "    # Pie chart for class distribution after SMOTE\n",
    "    axes[1].pie(\n",
    "        y_resampled_counts,\n",
    "        labels=y_resampled_counts.index,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        colors=sns.color_palette(\"muted\"),\n",
    "    )\n",
    "    axes[1].set_title(\"Class Distribution using SMOTE\", fontsize=12)\n",
    "\n",
    "    # Final layout adjustments\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Confusion Matrices and ROC Curves ---\n",
    "    _, axes = plt.subplots(2, 2, figsize=(18, 12))  # Increased figsize for larger matrices and curves\n",
    "\n",
    "    # Plot Confusion Matrices\n",
    "    disp_bootstrap = ConfusionMatrixDisplay(confusion_matrix=cm_bootstrap, display_labels=np.unique(y))\n",
    "    disp_bootstrap.plot(cmap=plt.cm.Blues, values_format=\"d\", ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(\"Confusion Matrix (Bootstrap)\", fontsize=14)\n",
    "    axes[0, 0].tick_params(axis='both', labelsize=12)  # Increase tick label size\n",
    "\n",
    "    disp_smote = ConfusionMatrixDisplay(confusion_matrix=cm_smote, display_labels=np.unique(y))\n",
    "    disp_smote.plot(cmap=plt.cm.Blues, values_format=\"d\", ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(\"Confusion Matrix (SMOTE)\", fontsize=14)\n",
    "    axes[0, 1].tick_params(axis='both', labelsize=12)  # Increase tick label size\n",
    "\n",
    "    # Plot ROC Curves\n",
    "    for i in range(y_test_bin_bootstrap.shape[1]):\n",
    "        fpr_bootstrap, tpr_bootstrap, _ = roc_curve(y_test_bin_bootstrap[:, i], predictions_bin_bootstrap[:, i])\n",
    "        roc_auc_bootstrap = auc(fpr_bootstrap, tpr_bootstrap)\n",
    "        axes[1, 0].plot(fpr_bootstrap, tpr_bootstrap, label=f\"Class {i} (AUC = {roc_auc_bootstrap:.2f})\")\n",
    "\n",
    "    for i in range(y_test_bin_smote.shape[1]):\n",
    "        fpr_smote, tpr_smote, _ = roc_curve(y_test_bin_smote[:, i], predictions_bin_smote[:, i])\n",
    "        roc_auc_smote = auc(fpr_smote, tpr_smote)\n",
    "        axes[1, 1].plot(fpr_smote, tpr_smote, label=f\"Class {i} (AUC = {roc_auc_smote:.2f})\")\n",
    "\n",
    "    # Random guess line for both ROC plots\n",
    "    for ax in [axes[1, 0], axes[1, 1]]:\n",
    "        ax.plot([0, 1], [0, 1], \"k--\", label=\"Random Guess\")\n",
    "        ax.set_xlabel(\"False Positive Rate\", fontsize=12)\n",
    "        ax.set_ylabel(\"True Positive Rate\", fontsize=12)\n",
    "        ax.legend(fontsize=10)\n",
    "\n",
    "    axes[1, 0].set_title(\"ROC Curve (Bootstrap)\", fontsize=14)\n",
    "    axes[1, 1].set_title(\"ROC Curve (SMOTE)\", fontsize=14)\n",
    "\n",
    "    # Final layout adjustments\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Display Classification Reports\n",
    "    display_reports_side_by_side(y_test, predictions_bootstrap, predictions_smote)\n",
    "    \n",
    "    print(\"=== Bootstrap Approach ===\")\n",
    "    print(f\"Execution Time: {elapsed_bootstrap:.2f} seconds\\n\")\n",
    "\n",
    "    print(\"=== SMOTE Approach ===\")\n",
    "    print(f\"Execution Time: {elapsed_smote:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **First Dataset - Wilt Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = imbalanced_datasets[0][\"Dataset ID\"]\n",
    "print(\"Dataset ID:\", dataset_id)\n",
    "print(\"Dataset Name:\", imbalanced_datasets[0][\"Dataset Name\"])\n",
    "data = openml.datasets.get_dataset(dataset_id)\n",
    "\n",
    "# Transform the dataset to a pandas DataFrame\n",
    "X, y, _, _ = data.get_data(\n",
    "    dataset_format=\"dataframe\", target=data.default_target_attribute\n",
    ")\n",
    "\n",
    "# Combine X and y for consistent row removal\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of missing values in the dataset\n",
    "print(\"Number of missing values:\", int(df.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"] = df[\"class\"].map({\"1\": False, \"2\": True})\n",
    "df[\"class\"] = df[\"class\"].astype(bool)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Evaluating both Random Forest Implementations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Second Dataset - Ozone Level Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = imbalanced_datasets[1][\"Dataset ID\"]\n",
    "print(\"Dataset ID:\", dataset_id)\n",
    "print(\"Dataset Name:\", imbalanced_datasets[1][\"Dataset Name\"])\n",
    "data = openml.datasets.get_dataset(dataset_id)\n",
    "\n",
    "# Transform the dataset to a pandas DataFrame\n",
    "X, y, _, _ = data.get_data(\n",
    "    dataset_format=\"dataframe\", target=data.default_target_attribute\n",
    ")\n",
    "\n",
    "# Combine X and y for consistent row removal\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of missing values in the dataset\n",
    "print(\"Number of missing values:\", int(df.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class\"] = df[\"Class\"].map({\"1\": 0, \"2\": 1})\n",
    "df[\"Class\"] = df[\"Class\"].astype(bool)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Evaluating both Random Forest Implementations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Third Dataset - Climate Model Simulation Crashes Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = imbalanced_datasets[2][\"Dataset ID\"]\n",
    "print(\"Dataset ID:\", dataset_id)\n",
    "print(\"Dataset Name:\", imbalanced_datasets[2][\"Dataset Name\"])\n",
    "data = openml.datasets.get_dataset(dataset_id)\n",
    "\n",
    "# Transform the dataset to a pandas DataFrame\n",
    "X, y, _, _ = data.get_data(\n",
    "    dataset_format=\"dataframe\", target=data.default_target_attribute\n",
    ")\n",
    "\n",
    "# Combine X and y for consistent row removal\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of missing values in the dataset\n",
    "print(\"Number of missing values:\", int(df.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"outcome\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"outcome\"] = df[\"outcome\"].map({\"0\": True, \"1\": False})\n",
    "df[\"outcome\"] = df[\"outcome\"].astype(bool)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Evaluating both Random Forest Implementations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Fourth Dataset - PC3 Software Defect Prediction Dataset**\n",
    "\n",
    "##### **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = imbalanced_datasets[3][\"Dataset ID\"]\n",
    "print(\"Dataset ID:\", dataset_id)\n",
    "print(\"Dataset Name:\", imbalanced_datasets[3][\"Dataset Name\"])\n",
    "data = openml.datasets.get_dataset(dataset_id)\n",
    "\n",
    "# Transform the dataset to a pandas DataFrame\n",
    "X, y, _, _ = data.get_data(\n",
    "    dataset_format=\"dataframe\", target=data.default_target_attribute\n",
    ")\n",
    "\n",
    "# Combine X and y for consistent row removal\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of missing values in the dataset\n",
    "print(\"Number of missing values:\", int(df.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"c\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Evaluating both Random Forest Implementations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
