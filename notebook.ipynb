{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def check_class_imbalance(threshold=0.8):\n",
    "    # Get datasets from OpenML Collection with ID 99\n",
    "    datasets = openml.study.get_suite(99)  # Suite ID 99\n",
    "\n",
    "    # List to store datasets with imbalance\n",
    "    imbalanced_datasets = []\n",
    "\n",
    "    print(\"Checking for class imbalance in OpenML Collection 99...\\n\")\n",
    "\n",
    "    for dataset_id in datasets.data:\n",
    "        try:\n",
    "            # Download the dataset\n",
    "            dataset = openml.datasets.get_dataset(dataset_id)\n",
    "            X, y, _, attributes = dataset.get_data(\n",
    "                target=dataset.default_target_attribute\n",
    "            )\n",
    "\n",
    "            # Ensure the target variable is categorical\n",
    "            if not isinstance(y, pd.Categorical):\n",
    "                y = pd.Categorical(y)\n",
    "\n",
    "            # Calculate class distribution\n",
    "            class_counts = y.value_counts()  # Absolute counts\n",
    "            total = class_counts.sum()  # Total number of instances\n",
    "            class_proportions = class_counts / total  # Normalized proportions\n",
    "            max_class_proportion = class_proportions.max()\n",
    "\n",
    "            # Check for imbalance\n",
    "            if max_class_proportion >= threshold:\n",
    "                imbalanced_datasets.append(\n",
    "                    {\n",
    "                        \"Dataset ID\": dataset_id,\n",
    "                        \"Dataset Name\": dataset.name,\n",
    "                        \"Max Class Proportion\": max_class_proportion,\n",
    "                        \"Class Distribution\": class_proportions.to_dict(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing dataset {dataset_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Sort the imbalanced datasets by max class proportion in descending order\n",
    "    imbalanced_datasets = sorted(\n",
    "        imbalanced_datasets, key=lambda x: x[\"Max Class Proportion\"], reverse=True\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    if imbalanced_datasets:\n",
    "        print(\"Datasets with class imbalance greater than the threshold:\\n\")\n",
    "        for dataset in imbalanced_datasets:\n",
    "            print(f\"Dataset ID: {dataset['Dataset ID']}\")\n",
    "            print(f\"Dataset Name: {dataset['Dataset Name']}\")\n",
    "            print(f\"Max Class Proportion: {dataset['Max Class Proportion']:.2%}\")\n",
    "            print(\"Class Distribution:\")\n",
    "            for cls, proportion in dataset[\"Class Distribution\"].items():\n",
    "                print(f\"  {cls}: {proportion:.2%}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No datasets found with class imbalance greater than the threshold.\")\n",
    "\n",
    "    return imbalanced_datasets\n",
    "\n",
    "\n",
    "# Run the script\n",
    "imbalanced_datasets = check_class_imbalance(threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(\n",
    "        self, feature=None, threshold=None, left=None, right=None, *, value=None\n",
    "    ):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def gini(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities**2)\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        best_gini = 1.0\n",
    "        best_feature, best_threshold = None, None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            X_feature = X[:, feature]\n",
    "            sorted_indices = np.argsort(X_feature)\n",
    "            X_sorted = X_feature[sorted_indices]\n",
    "            y_sorted = y[sorted_indices]\n",
    "\n",
    "            # Identify potential split points\n",
    "            for i in range(1, len(y_sorted)):\n",
    "                if y_sorted[i] != y_sorted[i - 1]:\n",
    "                    threshold = (X_sorted[i] + X_sorted[i - 1]) / 2\n",
    "                    left_mask = X_sorted < threshold\n",
    "                    y_left = y_sorted[left_mask]\n",
    "                    y_right = y_sorted[~left_mask]\n",
    "\n",
    "                    if len(y_left) == 0 or len(y_right) == 0:\n",
    "                        continue\n",
    "\n",
    "                    gini_left = self.gini(y_left)\n",
    "                    gini_right = self.gini(y_right)\n",
    "                    gini = (len(y_left) * gini_left + len(y_right) * gini_right) / len(\n",
    "                        y_sorted\n",
    "                    )\n",
    "\n",
    "                    if gini < best_gini:\n",
    "                        best_gini = gini\n",
    "                        best_feature = feature\n",
    "                        best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        num_samples, _ = X.shape\n",
    "        if (\n",
    "            self.max_depth is not None and depth >= self.max_depth\n",
    "        ) or num_samples < self.min_samples_split:\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return TreeNode(value=leaf_value)\n",
    "\n",
    "        feature, threshold = self.best_split(X, y)\n",
    "        if feature is None:\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return TreeNode(value=leaf_value)\n",
    "\n",
    "        # Split the data\n",
    "        left_indices = X[:, feature] < threshold\n",
    "        X_left, y_left = X[left_indices], y[left_indices]\n",
    "        X_right, y_right = X[~left_indices], y[~left_indices]\n",
    "\n",
    "        if len(y_left) == 0 or len(y_right) == 0:\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return TreeNode(value=leaf_value)\n",
    "\n",
    "        # Recursively build the left and right subtrees\n",
    "        left = self.build_tree(X_left, y_left, depth + 1)\n",
    "        right = self.build_tree(X_right, y_right, depth + 1)\n",
    "        return TreeNode(feature, threshold, left, right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.build_tree(X, y)\n",
    "\n",
    "    def predict_sample(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature] < node.threshold:\n",
    "            return self.predict_sample(x, node.left)\n",
    "        else:\n",
    "            return self.predict_sample(x, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_sample(x, self.root) for x in X])\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        max_features=\"sqrt\",\n",
    "        smote=False,\n",
    "        random_state=None,\n",
    "    ):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.smote = smote\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "\n",
    "    def bootstrap_sample(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        X_sample, y_sample = X[indices], y[indices]\n",
    "\n",
    "        # Ensure sampled data is still numeric\n",
    "        if X_sample.dtype == \"O\" or y_sample.dtype == \"O\":\n",
    "            X_sample, y_sample = convert_to_numeric(X_sample, y_sample)\n",
    "\n",
    "        return X_sample, y_sample\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert to NumPy arrays if they are pandas structures\n",
    "        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "\n",
    "        # Convert to numeric if necessary\n",
    "        if X.dtype == \"O\" or y.dtype == \"O\":\n",
    "            X, y = convert_to_numeric(X, y)\n",
    "\n",
    "        # Apply SMOTE if enabled\n",
    "        if self.smote:\n",
    "            sm = SMOTE(sampling_strategy=0.2, random_state=self.random_state)\n",
    "            # sm = RandomUnderSampler(random_state=self.random_state)\n",
    "            # sm = SMOTEENN(random_state=self.random_state)\n",
    "            # sm = SMOTETomek(random_state=self.random_state)\n",
    "            # sm = ADASYN(random_state=self.random_state)\n",
    "            X, y = sm.fit_resample(X, y)\n",
    "\n",
    "        self.trees = []\n",
    "        n_features_total = X.shape[1]\n",
    "\n",
    "        # Determine the number of features to consider at each split\n",
    "        if self.max_features == \"sqrt\":\n",
    "            max_features = int(np.sqrt(n_features_total))\n",
    "        elif self.max_features == \"log2\":\n",
    "            max_features = int(np.log2(n_features_total))\n",
    "        elif isinstance(self.max_features, int):\n",
    "            max_features = self.max_features\n",
    "        elif self.max_features.lower() == \"auto\":\n",
    "            max_features = n_features_total\n",
    "        else:\n",
    "            raise ValueError(\"RandomForest.fit: Invalid value for max_features\")\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTree(\n",
    "                max_depth=self.max_depth, min_samples_split=self.min_samples_split\n",
    "            )\n",
    "            X_sample, y_sample = self.bootstrap_sample(X, y)\n",
    "            \n",
    "            # Select random subset of features\n",
    "            features = np.random.choice(n_features_total, max_features, replace=False)\n",
    "\n",
    "            # Fit the tree on the sampled data with selected features\n",
    "            tree.fit(X_sample[:, features], y_sample)\n",
    "\n",
    "            tree.features = features\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Convert to NumPy array if it's a pandas structure\n",
    "        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "            X = X.values\n",
    "\n",
    "        tree_preds = []\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            try:\n",
    "                preds = tree.predict(X[:, tree.features])\n",
    "                tree_preds.append(preds)\n",
    "            except Exception as e:\n",
    "                print(f\"RandomForest.predict: Error predicting with tree {i+1}: {e}\")\n",
    "                raise e\n",
    "\n",
    "        tree_preds = np.array(tree_preds).T  # Shape: (n_samples, n_estimators)\n",
    "        y_pred = [Counter(row).most_common(1)[0][0] for row in tree_preds]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "\n",
    "# Helper function\n",
    "def convert_to_numeric(X, y):\n",
    "    # Convert X to float\n",
    "    if X.dtype == \"O\":\n",
    "        X = X.astype(float)\n",
    "\n",
    "    # Convert y to integer\n",
    "    if y.dtype == \"O\":\n",
    "        y = y.astype(int)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import openml  # Ensure openml is imported\n",
    "\n",
    "# Define your custom RandomForest class here or ensure it's imported\n",
    "# from your_random_forest_module import RandomForest\n",
    "\n",
    "# Load the datasets from OpenML Collection 99\n",
    "dataset_id = imbalanced_datasets[6][\"Dataset ID\"]\n",
    "print(\"Dataset ID:\", dataset_id)\n",
    "data = openml.datasets.get_dataset(dataset_id)\n",
    "\n",
    "X, y, _, _ = data.get_data(\n",
    "    dataset_format=\"dataframe\", target=data.default_target_attribute\n",
    ")\n",
    "\n",
    "# Combine X and y for consistent row removal\n",
    "combined_data = pd.concat([X, y], axis=1)\n",
    "\n",
    "combined_data_clean = combined_data.dropna()\n",
    "\n",
    "# Separate X_clean and y_clean\n",
    "X_clean = combined_data_clean.drop(\n",
    "    columns=[y.name]\n",
    ")  # Replace y.name with the actual target column name if different\n",
    "y_clean = combined_data_clean[y.name]\n",
    "\n",
    "# Update X and y to cleaned versions\n",
    "X, y = X_clean, y_clean\n",
    "\n",
    "# Identify non-numeric columns in X\n",
    "non_numeric_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "# Encode categorical features by converting them to categorical codes\n",
    "if len(non_numeric_cols) > 0:\n",
    "    for col in non_numeric_cols:\n",
    "        X[col] = X[col].astype(\"category\").cat.codes\n",
    "        print(f\"\\nEncoded '{col}' with codes:\\n\", X[col].unique())\n",
    "    print(\"\\nAfter Encoding Feature dtypes:\\n\", X.dtypes)\n",
    "else:\n",
    "    print(\"\\nNo non-numeric features to encode.\")\n",
    "\n",
    "# Encode target variable using Pandas Categorical Codes if it's non-numeric\n",
    "if y.dtype == \"object\" or y.dtype.name == \"category\":\n",
    "    y = y.astype(\"category\")\n",
    "    y_codes = y.cat.codes\n",
    "    print(\"\\nEncoded Target dtype:\", y_codes.dtype)\n",
    "    print(\n",
    "        \"Target Categories and Codes:\\n\", dict(zip(y.cat.categories, y_codes.unique()))\n",
    "    )\n",
    "    y = y_codes\n",
    "\n",
    "max_accuracy = 0\n",
    "max_random_state = 0\n",
    "\n",
    "random_state = 99\n",
    "\n",
    "# Split the data with stratification to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=random_state, stratify=y\n",
    ")\n",
    "\n",
    "# Initialize RandomForest with SMOTE\n",
    "rf = RandomForest(\n",
    "    n_estimators=10,\n",
    "    max_depth=5,\n",
    "    smote=True,\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.sum(predictions == y_test) / len(y_test)\n",
    "print(f\"\\nSMOTE Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "rf = RandomForest(\n",
    "    n_estimators=10,\n",
    "    max_depth=5,\n",
    "    smote=False,\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.sum(predictions == y_test) / len(y_test)\n",
    "print(f\"\\nWithout SMOTE Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
